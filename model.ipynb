{"cells":[{"cell_type":"markdown","source":["# Model structure"],"metadata":{"id":"4L52WbQF0Crj"}},{"cell_type":"markdown","source":["<img src='https://drive.google.com/uc?id=1xq5YGrLMtNO4lxMw7sWa2VoQXgDpx-y0' width=\"800\"/>\n"],"metadata":{"id":"OQQjioYF-7CI"}},{"cell_type":"code","source":["class CARE_Net(nn.Module):\n","    def __init__(self, c_in=2, c_out=1, embed_dim=256, dim = 64, device=\"cuda\"):\n","        super().__init__()\n","\n","\n","        self.device = device\n","        self.embed_dim = embed_dim\n","\n","        self.inc = nn.Sequential(nn.Conv2d(c_in, dim, kernel_size=3, padding=1, bias=False),\n","                                LayerNorm2d(dim, elementwise_affine=False, eps=1e-6))\n","        self.inc_block1 = Double_Convnext(dim,embed_dim)\n","        self.sa0 = LinearAttention(dim)\n","\n","        self.down1 = Down(dim, dim*2, embed_dim)\n","        self.sa1 = LinearAttention(dim*2)\n","\n","        self.down2 = Down(dim*2, dim*4, embed_dim)\n","        self.sa2 = LinearAttention(dim*4)\n","\n","        self.down3 = nn.Sequential(\n","                    LayerNorm2d(dim*4, elementwise_affine=False, eps=1e-6),\n","                    nn.Conv2d(dim*4, dim*8, kernel_size=2, stride=2),)\n","\n","        self.bot1_1 = ResBlock(dim*8)\n","        self.bot1_2 = ResBlock(dim*8)\n","        self.bot_sa1 = Attention(dim*8)\n","\n","        self.bot2_1 = ResBlock(dim*8)\n","        self.bot2_2 = ResBlock(dim*8)\n","        self.bot_sa2 = Attention(dim*8)\n","\n","        self.bot3_1 = ResBlock(dim*8)\n","        self.bot3_2 = ResBlock(dim*8)\n","        self.bot_sa3 = Attention(dim*8)\n","\n","        self.up1 = Up(dim*8, dim*4, dim*4, embed_dim)\n","        self.sa4 = LinearAttention(dim*4)\n","\n","        self.up2 = Up(dim*4, dim*2, dim*2, embed_dim)\n","        self.sa5 = LinearAttention(dim*2)\n","\n","        self.up3 = Up(dim*2, dim, dim, embed_dim)\n","        self.sa6 = LinearAttention(dim)\n","\n","        self.outc = nn.Conv2d(dim, c_out, kernel_size=1)\n","\n","    def pos_encoding(self, t, channels):\n","        inv_freq = 1.0 / (\n","            10000\n","            ** (torch.arange(0, channels, 2, device=self.device).float() / channels)\n","        )\n","        pos_enc_a = torch.sin(t.repeat(1, channels // 2) * inv_freq)\n","        pos_enc_b = torch.cos(t.repeat(1, channels // 2) * inv_freq)\n","        pos_enc = torch.cat([pos_enc_a, pos_enc_b], dim=-1)\n","        return pos_enc\n","\n","\n","    def forward(self, x,  t):\n","\n","        t = t.unsqueeze(-1).type(torch.float)\n","        t = self.pos_encoding(t, self.embed_dim)\n","\n","\n","        x1 = self.inc(x)\n","        x1 = self.inc_block1(x1,t)\n","        x1 = self.sa0(x1)\n","\n","        x2 = self.down1(x1, t)\n","        x2 = self.sa1(x2)\n","\n","        x3 = self.down2(x2, t)\n","        x3 = self.sa2(x3)\n","\n","        x4 = self.down3(x3)\n","        x4 = self.bot1_1(x4)\n","        x4 = self.bot1_2(x4)\n","        x4 = self.bot_sa1(x4)\n","\n","        x4 = self.bot2_1(x4)\n","        x4 = self.bot2_2(x4)\n","        x4 = self.bot_sa2(x4)\n","\n","        x4 = self.bot3_1(x4)\n","        x4 = self.bot3_2(x4)\n","        x4 = self.bot_sa3(x4)\n","\n","        x = self.up1(x4, t, x3)\n","        x = self.sa4(x)\n","\n","        x = self.up2(x, t, x2)\n","        x = self.sa5(x)\n","\n","        x = self.up3(x, t,  x1)\n","        x = self.sa6(x)\n","\n","        output = self.outc(x)\n","        return output"],"metadata":{"id":"Vh1aGdIy8Vuj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[" # ConvNeXt block"],"metadata":{"id":"yiANdybk-hPO"}},{"cell_type":"code","source":["class ResBlock(nn.Module):\n","    def __init__(self, c, c_emb = None, c_skip=0, kernel_size=7, dropout=0.0):\n","        super().__init__()\n","        self.depthwise = nn.Conv2d(c + c_skip, c, kernel_size=kernel_size, padding=kernel_size // 2, groups=c)\n","        self.norm = LayerNorm2d(c, elementwise_affine=False, eps=1e-6)\n","        self.channelwise = nn.Sequential(\n","            nn.Linear(c, c * 4),\n","            nn.GELU(),\n","            GlobalResponseNorm(c * 4),\n","            nn.Dropout(dropout),\n","            nn.Linear(c * 4, c)\n","        )\n","        self.mlp = nn.Sequential(\n","            nn.GELU(),\n","            nn.Linear(c_emb, c)\n","        ) if exists(c_emb) else None\n","\n","    def forward(self, x, t=None, x_skip=None):\n","        x_res = x\n","        if x_skip is not None:\n","            x = torch.cat([x, x_skip], dim=1)\n","        x = self.depthwise(x)\n","        if t is not None:\n","            emb = self.mlp(t)[:, :, None, None].repeat(1, 1, x.shape[-2], x.shape[-1])\n","            x = x + emb\n","        x = self.norm(x).permute(0, 2, 3, 1)\n","        x = self.channelwise(x).permute(0, 3, 1, 2)\n","        return x + x_res\n"],"metadata":{"id":"S5lWkR7U-VK1"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}